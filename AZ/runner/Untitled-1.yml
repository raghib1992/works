apiVersion: v1
kind: Pod
metadata:
  finalizers:
    - actions.summerwind.dev/runner-pod
  labels:
    actions-runner: ''
    actions-runner-controller/inject-registration-token: 'true'
    pod-template-hash: 5c8c95cb6
    runner-deployment-name: runner-deploy-azu-ignite-azimuth-profiles
    runner-template-hash: 5b84488ccb
  name: runner-deploy-azu-ignite-azimuth-profiles-dv2jk-p6wck
  namespace: brown-dev-001
  ownerReferences:
    - apiVersion: actions.summerwind.dev/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: Runner
      name: runner-deploy-azu-ignite-azimuth-profiles-dv2jk-p6wck
      uid: 57faf4d1-2cc4-4599-bc67-0538b37d02d6
  resourceVersion: '2576915108'
  uid: 5d9bd7fa-100a-473a-a91b-89a1237346ae
spec:
  containers:
    - env:
        - name: http_proxy
          value: 'http://azpse.astrazeneca.net:9480'
        - name: https_proxy
          value: 'http://azpse.astrazeneca.net:9480'
        - name: no_proxy
          value: '10.0.0.0/8,172.29.0.0/8,astrazeneca.net'
        - name: DISABLE_RUNNER_UPDATE
          value: 'true'
        - name: RUNNER_ORG
        - name: RUNNER_REPO
          value: azu-ignite/azimuth-profiles
        - name: RUNNER_ENTERPRISE
        - name: RUNNER_LABELS
          value: azimuth-azimuth-profiles
        - name: RUNNER_GROUP
        - name: DOCKER_ENABLED
          value: 'true'
        - name: DOCKERD_IN_RUNNER
          value: 'false'
        - name: GITHUB_URL
          value: 'https://github.com/'
        - name: RUNNER_WORKDIR
          value: /runner/_work
        - name: RUNNER_EPHEMERAL
          value: 'true'
        - name: RUNNER_STATUS_UPDATE_HOOK
          value: 'false'
        - name: GITHUB_ACTIONS_RUNNER_EXTRA_USER_AGENT
          value: actions-runner-controller/v0.27.6
        - name: DOCKER_HOST
          value: 'unix:///run/docker.sock'
        - name: RUNNER_NAME
          value: runner-deploy-azu-ignite-azimuth-profiles-dv2jk-p6wck
        - name: RUNNER_TOKEN
          value: BCZY3JGDLE4VSPZNUSDW3LTHVHO7K
      image: >-
        harbor.csis.astrazeneca.net/summerwind/actions-runner:v2.321.0-ubuntu-22.04
      imagePullPolicy: Always
      name: runner
      resources:
        limits:
          cpu: '1'
          memory: 3Gi
        requests:
          cpu: '1'
          memory: 3Gi
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /runner
          name: runner
        - mountPath: /runner/_work
          name: work
        - mountPath: /run
          name: var-run
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-975h5
          readOnly: true
    - args:
        - dockerd
        - '--host=unix:///run/docker.sock'
        - '--group=$(DOCKER_GROUP_GID)'
      env:
        - name: http_proxy
          value: 'http://azpse.astrazeneca.net:9480'
        - name: https_proxy
          value: 'http://azpse.astrazeneca.net:9480'
        - name: no_proxy
          value: '10.0.0.0/8,172.29.0.0/8,astrazeneca.net'
        - name: DOCKER_GROUP_GID
          value: '121'
      image: 'harbor.csis.astrazeneca.net/docker/nvdia-dind:latest'
      imagePullPolicy: Always
      lifecycle:
        preStop:
          exec:
            command:
              - /bin/sh
              - '-c'
              - >-
                timeout "${RUNNER_GRACEFUL_STOP_TIMEOUT:-15}" /bin/sh -c "echo
                'Prestop hook started'; while [ -f /runner/.runner ]; do sleep
                1; done; echo 'Waiting for dockerd to start'; while ! pgrep -x
                dockerd; do sleep 1; done; echo 'Prestop hook stopped'"
                >/proc/1/fd/1 2>&1
      name: docker
      resources:
        limits:
          cpu: 200m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 512Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /runner
          name: runner
        - mountPath: /run
          name: var-run
        - mountPath: /runner/_work
          name: work
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-975h5
          readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  volumes:
    - emptyDir: {}
      name: runner
    - emptyDir: {}
      name: work
    - emptyDir:
        medium: Memory
        sizeLimit: 1M
      name: var-run
    - name: kube-api-access-975h5
      projected:
        defaultMode: 420
        sources:
          - serviceAccountToken:
              expirationSeconds: 3607
              path: token
          - configMap:
              items:
                - key: ca.crt
                  path: ca.crt
              name: kube-root-ca.crt
          - downwardAPI:
              items:
                - fieldRef:
                    apiVersion: v1
                    fieldPath: metadata.namespace
                  path: namespace
status:
  conditions:
    - lastProbeTime: null
      lastTransitionTime: '2025-02-10T10:07:33Z'
      message: >-
        0/13 nodes are available: 2 node(s) had untolerated taint {OnlyNginx:
        true}, 3 node(s) had untolerated taint
        {node-role.kubernetes.io/control-plane: }, 8 Insufficient cpu.
        preemption: 0/13 nodes are available: 5 Preemption is not helpful for
        scheduling, 8 No preemption victims found for incoming pod..
      reason: Unschedulable
      status: 'False'
      type: PodScheduled
  phase: Pending
  qosClass: Burstable
